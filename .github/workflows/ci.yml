name: Build

on:
  push:
    branches:
      - main
  pull_request_target:
    branches:
      - main

jobs:
  code-quality:
    name: Code Quality Checks
    runs-on: ubuntu-latest
    env:
      CARGO_TERM_COLOR: always
    permissions: {}
    outputs:
      rust-fmt-result: ${{ steps.rust-fmt.outcome }}
      rust-lint-result: ${{ steps.rust-lint.outcome }}
      prettier-fmt-result: ${{ steps.prettier-fmt.outcome }}
      eslint-result: ${{ steps.eslint.outcome }}
    steps:
      - name: Checkout base branch
        if: ${{ github.event_name == 'push' }}
        uses: actions/checkout@v5

      - name: Checkout merge commit
        if: ${{ github.event_name == 'pull_request_target' }}
        uses: actions/checkout@v5
        with:
          ref: ${{ github.event.pull_request.head.ref }}

      # Speed up apt-get operations by disabling unnecessary man-db auto-update
      - name: Disable man-db auto-update
        run: |
          echo "man-db man-db/auto-update boolean false" | sudo debconf-set-selections
          sudo rm -f /var/lib/man-db/auto-update

      # Install system dependencies for Linux printing
      - name: Install CUPS development libraries
        run: |
          sudo apt-get update
          sudo apt-get install -y libcups2-dev pkg-config clang

      # Setup Rust toolchain for building native library
      - name: Setup Rust
        uses: actions-rust-lang/setup-rust-toolchain@v1

      # Cache Rust dependencies for faster builds
      - uses: Swatinem/rust-cache@v2
        with:
          prefix-key: ${{ runner.os }}-rust

      # Install cargo-llvm-cov for code coverage
      - name: Install cargo-llvm-cov
        uses: taiki-e/install-action@cargo-llvm-cov

      # Install cargo2junit for JUnit XML output from Cargo tests
      - name: Install cargo2junit
        run: cargo install cargo2junit

      # Setup Deno
      - uses: denoland/setup-deno@v2
        with:
          deno-version: v2.x # Run with latest stable Deno.
          cache: true

      # Setup Node.js for N-API module and testing
      - name: Setup Node.js
        uses: actions/setup-node@v5
        with:
          node-version: "20"
          cache: "npm"

      - name: Install NAPI-RS CLI
        run: npm install -g @napi-rs/cli

      # Setup Bun for cross-runtime testing
      - name: Setup Bun
        uses: oven-sh/setup-bun@v2
        with:
          bun-version: latest

      # Install Task runner
      - name: Setup Task
        uses: arduino/setup-task@v2
        with:
          version: 3.x
          repo-token: ${{ secrets.GITHUB_TOKEN }}

      # Code quality checks (continue on error to gather all results)
      - name: Check Rust formatting
        id: rust-fmt
        run: task fmt:rust:check
        continue-on-error: true

      - name: Run Rust linter (Clippy)
        id: rust-lint
        run: task lint:rust
        continue-on-error: true

      - name: Check TypeScript/JavaScript formatting
        id: prettier-fmt
        run: task fmt:prettier:check
        continue-on-error: true


      # Install Node.js dependencies
      - name: Install Node.js dependencies
        run: npm install

      - name: Run ESLint
        id: eslint
        run: task lint:eslint
        continue-on-error: true

  build-and-test:
    name: Build and Test (${{ matrix.os }})
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        include:
          - os: ubuntu-latest
            name: Linux
          - os: macos-latest
            name: macOS
          - os: windows-latest
            name: Windows
    env:
      PRINTERS_JS_SIMULATE: true # Force simulation mode for all tests
      CARGO_TERM_COLOR: always
    permissions: {}
    steps:
      - name: Checkout base branch
        if: ${{ github.event_name == 'push' }}
        uses: actions/checkout@v5

      - name: Checkout merge commit
        if: ${{ github.event_name == 'pull_request_target' }}
        uses: actions/checkout@v5
        with:
          ref: ${{ github.event.pull_request.head.ref }}

      # Linux-specific setup
      - name: Disable man-db auto-update and install CUPS libraries (Linux)
        if: runner.os == 'Linux'
        run: |
          echo "man-db man-db/auto-update boolean false" | sudo debconf-set-selections
          sudo rm -f /var/lib/man-db/auto-update
          sudo apt-get update
          sudo apt-get install -y libcups2-dev pkg-config clang

      # Setup Rust toolchain for building native library
      - name: Setup Rust
        uses: actions-rust-lang/setup-rust-toolchain@v1

      # Cache Rust dependencies for faster builds
      - uses: Swatinem/rust-cache@v2
        with:
          prefix-key: ${{ matrix.os }}-rust

      # Install cargo-llvm-cov for code coverage (Linux only)
      - name: Install cargo-llvm-cov (Linux only)
        if: runner.os == 'Linux'
        uses: taiki-e/install-action@cargo-llvm-cov

      # Setup Deno
      - uses: denoland/setup-deno@v2
        with:
          deno-version: v2.x

      # Setup Bun
      - uses: oven-sh/setup-bun@v2
        with:
          bun-version: latest

      # Setup Task runner
      - uses: arduino/setup-task@v2
        with:
          version: 3.x
          repo-token: ${{ secrets.GITHUB_TOKEN }}

      # Setup Node.js
      - name: Setup Node
        uses: actions/setup-node@v5
        with:
          node-version: 20

      # Install Node.js dependencies
      - name: Install Node.js dependencies
        run: npm install

      # Build all runtime libraries (includes compilation)
      - name: Build all runtime libraries
        run: task build

      # Run Cargo tests with coverage (Linux only)
      - name: Run Cargo tests with coverage (Linux only)
        if: runner.os == 'Linux'
        id: cargo-tests-coverage
        run: |
          mkdir -p test-results/coverage
          cargo llvm-cov --all-features --workspace --lcov --output-path test-results/coverage/rust.lcov test
        continue-on-error: true

      # Run Cargo tests (all platforms)
      - name: Run Cargo tests
        id: cargo-tests
        shell: bash
        run: |
          mkdir -p test-results
          # Run cargo tests and capture output
          cargo test --all-features --workspace > cargo_test_output.txt 2>&1 || true

      # Generate JUnit report from Cargo test output
      - name: Generate Cargo test JUnit report
        id: cargo-junit
        shell: bash
        run: |

          # Parse the test results from cargo output
          # Look for lines like "test result: ok. 16 passed; 0 failed; 0 ignored"
          RESULT_LINE=$(grep "test result:" cargo_test_output.txt | head -1)

          if [ -n "$RESULT_LINE" ]; then
            echo "Found test result line: $RESULT_LINE"

            # Extract numbers using more robust parsing
            PASSED=$(echo "$RESULT_LINE" | grep -oE "[0-9]+ passed" | grep -oE "[0-9]+" || echo "0")
            FAILED=$(echo "$RESULT_LINE" | grep -oE "[0-9]+ failed" | grep -oE "[0-9]+" || echo "0")
            IGNORED=$(echo "$RESULT_LINE" | grep -oE "[0-9]+ ignored" | grep -oE "[0-9]+" || echo "0")

            # Calculate total tests
            TOTAL=$((PASSED + FAILED + IGNORED))

            echo "Parsed: Total=$TOTAL, Passed=$PASSED, Failed=$FAILED, Ignored=$IGNORED"
          else
            echo "No test result line found, using defaults"
            TOTAL=0
            PASSED=0
            FAILED=0
            IGNORED=0
          fi

          # Generate JUnit XML
          echo '<?xml version="1.0" encoding="UTF-8"?>' > test-results/cargo.xml
          echo '<testsuites>' >> test-results/cargo.xml
          echo "  <testsuite name=\"Rust Cargo Tests\" tests=\"${TOTAL}\" failures=\"${FAILED}\" errors=\"0\" skipped=\"${IGNORED}\">" >> test-results/cargo.xml

          # Add individual test cases from the output
          grep "^test " cargo_test_output.txt | while IFS= read -r line; do
            # Extract test name and status
            TEST_NAME=$(echo "$line" | sed 's/test \(.*\) \.\.\. .*/\1/')
            TEST_STATUS=$(echo "$line" | sed 's/.*\.\.\. \(.*\)/\1/')

            if [ "$TEST_STATUS" = "ok" ]; then
              echo "    <testcase name=\"${TEST_NAME}\" classname=\"cargo\" />" >> test-results/cargo.xml
            elif [ "$TEST_STATUS" = "FAILED" ]; then
              echo "    <testcase name=\"${TEST_NAME}\" classname=\"cargo\">" >> test-results/cargo.xml
              echo "      <failure message=\"Test failed\" />" >> test-results/cargo.xml
              echo "    </testcase>" >> test-results/cargo.xml
            fi
          done

          echo '  </testsuite>' >> test-results/cargo.xml
          echo '</testsuites>' >> test-results/cargo.xml

          echo "Generated JUnit XML with $TOTAL tests"
          cat test-results/cargo.xml
        continue-on-error: true

      # Run comprehensive cross-runtime tests using our TypeScript script
      - name: Run cross-runtime tests
        id: cross-runtime-tests
        run: task test
        continue-on-error: true

      - name: Upload Test Reports
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: test-reports-${{ matrix.name }}
          path: test-results/*.xml
          if-no-files-found: ignore

      - name: Upload Coverage Reports (Linux only)
        if: ${{ always() && runner.os == 'Linux' }}
        uses: actions/upload-artifact@v4
        with:
          name: coverage-reports
          path: test-results/coverage/*
          if-no-files-found: ignore

  report:
    if: ${{ always() && github.event_name == 'pull_request_target'}}
    runs-on: ubuntu-latest
    needs: [code-quality, build-and-test]
    permissions:
      contents: read
      pull-requests: write
      actions: read
      checks: write
      statuses: write
    steps:
      # Speed up apt-get operations by disabling unnecessary man-db auto-update
      - name: Disable man-db auto-update
        run: |
          echo "man-db man-db/auto-update boolean false" | sudo debconf-set-selections
          sudo rm -f /var/lib/man-db/auto-update

      # Download test reports from each platform separately
      - uses: actions/download-artifact@v5
        if: ${{ always() }}
        continue-on-error: true
        with:
          name: test-reports-Linux
          path: reports/Linux

      - uses: actions/download-artifact@v5
        if: ${{ always() }}
        continue-on-error: true
        with:
          name: test-reports-macOS
          path: reports/macOS

      - uses: actions/download-artifact@v5
        if: ${{ always() }}
        continue-on-error: true
        with:
          name: test-reports-Windows
          path: reports/Windows

      # Download coverage reports (only from Linux)
      - uses: actions/download-artifact@v5
        if: ${{ always() }}
        continue-on-error: true
        with:
          name: coverage-reports
          path: coverage-reports

      # Create combined coverage report from all runtimes
      - name: Combine coverage reports
        run: |
          # Coverage files are already organized in coverage-reports directory

          # Create combined report from coverage files
          if [ -f coverage-reports/rust.lcov ]; then
            cp coverage-reports/rust.lcov coverage-reports/combined.lcov
          else
            touch coverage-reports/combined.lcov
          fi

          if [ -f coverage-reports/deno-lcov.info ]; then
            cat coverage-reports/deno-lcov.info >> coverage-reports/combined.lcov
          fi

          # Clean up the combined coverage file to remove problematic entries
          # This filters out TLA:GNC lines which can cause genhtml errors
          if [ -f coverage-reports/combined.lcov ] && [ -s coverage-reports/combined.lcov ]; then
            cd coverage-reports
            grep -v "TLA:GNC" combined.lcov > combined_clean.lcov || cp combined.lcov combined_clean.lcov
            mv combined_clean.lcov combined.lcov
            cd ..
          fi

      # Add/update dynamic test summary comment based on actual results
      - name: Update Test Summary Comment
        uses: actions/github-script@v8
        with:
          script: |
            const fs = require('fs');
            const path = require('path');

            // Function to parse JUnit XML (handles different formats)
            function parseJUnit(xmlContent) {
              // Try multiple patterns for different JUnit formats
              // Pattern 1: Standard JUnit with tests, errors, failures attributes
              let testMatch = xmlContent.match(/<testsuite[^>]*tests="(\d+)"[^>]*errors="(\d+)"[^>]*failures="(\d+)"/);

              // Pattern 2: Alternative format with different attribute order
              if (!testMatch) {
                const testsMatch = xmlContent.match(/tests="(\d+)"/);
                const errorsMatch = xmlContent.match(/errors="(\d+)"/);
                const failuresMatch = xmlContent.match(/failures="(\d+)"/);

                if (testsMatch) {
                  const tests = parseInt(testsMatch[1]);
                  const errors = errorsMatch ? parseInt(errorsMatch[1]) : 0;
                  const failures = failuresMatch ? parseInt(failuresMatch[1]) : 0;
                  return {
                    total: tests,
                    errors: errors,
                    failures: failures,
                    passed: tests - errors - failures
                  };
                }
              }

              // Pattern 3: Count individual testcase elements
              if (!testMatch) {
                const testcases = (xmlContent.match(/<testcase/g) || []).length;
                const failures = (xmlContent.match(/<failure/g) || []).length;
                const errors = (xmlContent.match(/<error/g) || []).length;

                if (testcases > 0) {
                  return {
                    total: testcases,
                    errors: errors,
                    failures: failures,
                    passed: testcases - errors - failures
                  };
                }
              }

              if (testMatch) {
                return {
                  total: parseInt(testMatch[1]),
                  errors: parseInt(testMatch[2]),
                  failures: parseInt(testMatch[3]),
                  passed: parseInt(testMatch[1]) - parseInt(testMatch[2]) - parseInt(testMatch[3])
                };
              }

              return { total: 0, errors: 0, failures: 0, passed: 0 };
            }

            // Function to format test result
            function formatTestResult(result) {
              const total = result.passed + result.failures + result.errors;
              const status = result.failures === 0 && result.errors === 0 ? '✅' : '❌';

              if (result.failures === 0 && result.errors === 0 && result.passed === result.total) {
                return { status, text: `${result.total}/${result.total} passed (all platforms)` };
              } else {
                return { status, text: `${result.passed}/${result.total} passed` };
              }
            }

            // Parse LCOV coverage file to get coverage percentage
            function parseLCOVCoverage(filePath) {
              try {
                if (!fs.existsSync(filePath)) {
                  return 'No coverage';
                }

                const lcovContent = fs.readFileSync(filePath, 'utf8');

                // Count total and hit lines
                let totalLines = 0;
                let hitLines = 0;

                const lines = lcovContent.split('\n');
                for (const line of lines) {
                  if (line.startsWith('DA:')) {
                    // DA:line_number,hit_count
                    totalLines++;
                    const hitCount = parseInt(line.split(',')[1] || '0');
                    if (hitCount > 0) {
                      hitLines++;
                    }
                  }
                }

                if (totalLines === 0) {
                  return 'No data';
                }

                const percentage = Math.round((hitLines / totalLines) * 100);
                return `${percentage}% (${hitLines}/${totalLines})`;
              } catch (error) {
                console.log('Error parsing LCOV:', error.message);
                return 'Parse error';
              }
            }

            // Generate the summary content
            let summary = `## 🧪 Test Results\n\n`;

            summary += `### JavaScript Runtime Tests\n`;
            summary += `Test suite (**14 tests**) verified across **3 JavaScript runtimes** on **3 platforms**:\n\n`;
            summary += `| Runtime | Status | Suite Results | Coverage |\n`;
            summary += `|---------|--------|-------|----------|\n`;

            // Aggregate test results from matrix job platforms
            const platformResults = {};
            const results = {};

            // Debug: List available platform folders and their test files
            console.log('Checking platform-specific test results:');
            const platforms = ['Linux', 'macOS', 'Windows'];
            const platformFiles = {};

            for (const platform of platforms) {
              const platformPath = `reports/${platform}`;
              try {
                platformFiles[platform] = fs.readdirSync(platformPath).filter(f => f.endsWith('.xml'));
                console.log(`${platform}: ${platformFiles[platform].join(', ')}`);
              } catch (error) {
                platformFiles[platform] = [];
                console.log(`${platform}: No files (${error.message})`);
              }
            }

            // Get outcomes from build-and-test matrix jobs
            const matrixResults = ${{ toJSON(needs.build-and-test.result) }};
            console.log('Matrix job result:', matrixResults);

            // Initialize runtime results aggregation
            const runtimes = ['deno', 'bun', 'node', 'rust'];
            for (const runtime of runtimes) {
              results[runtime] = {
                platforms: {},
                aggregated: { success: false, total: 0, errors: 0, failures: 0, passed: 0 }
              };
            }

            // Process test files from each platform
            for (const platform of platforms) {
              console.log(`Processing ${platform} test results:`);

              for (const fileName of platformFiles[platform]) {
                const filePath = `reports/${platform}/${fileName}`;
                console.log(`  Processing: ${filePath}`);
                try {
                  const xml = fs.readFileSync(filePath, 'utf8');
                  const parsed = parseJUnit(xml);

                  // Determine which runtime this file belongs to
                  let runtime = null;
                  if (fileName.includes('deno')) runtime = 'deno';
                  else if (fileName.includes('bun')) runtime = 'bun';
                  else if (fileName.includes('node')) runtime = 'node';
                  else if (fileName.includes('cargo')) runtime = 'rust';

                  if (runtime) {
                    // Store platform-specific results
                    if (!results[runtime].platforms[platform]) {
                      results[runtime].platforms[platform] = { success: false, total: 0, errors: 0, failures: 0, passed: 0 };
                    }

                    const platformResults = results[runtime].platforms[platform];
                    platformResults.total += parsed.total;
                    platformResults.errors += parsed.errors;
                    platformResults.failures += parsed.failures;
                    platformResults.passed += parsed.passed;
                    platformResults.success = platformResults.failures === 0 && platformResults.errors === 0;

                    // Don't aggregate across platforms yet - we'll do proper aggregation after processing all platforms

                    console.log(`  ${runtime} on ${platform}: ${parsed.passed}/${parsed.total} passed`);
                  }
                } catch (error) {
                  console.log(`  Error processing ${filePath}:`, error.message);
                }
              }
            }

            // Calculate aggregated results properly - don't sum across platforms,
            // instead determine if all platforms passed their tests
            for (const runtime of runtimes) {
              const platformKeys = Object.keys(results[runtime].platforms);
              if (platformKeys.length > 0) {
                // Use the test count from the first platform (all should be the same)
                const firstPlatform = platformKeys[0];
                const referenceResults = results[runtime].platforms[firstPlatform];

                // Check if ALL platforms passed their tests
                const allPlatformsPassed = platformKeys.every(platform =>
                  results[runtime].platforms[platform].success
                );

                results[runtime].aggregated = {
                  total: referenceResults.total,
                  errors: allPlatformsPassed ? 0 : 1,
                  failures: allPlatformsPassed ? 0 : 0,
                  passed: allPlatformsPassed ? referenceResults.total : referenceResults.total - 1,
                  success: allPlatformsPassed
                };
              }
            }

            // Fallback: if no results found, mark as failed
            for (const runtime of runtimes) {
              if (results[runtime].aggregated.total === 0) {
                results[runtime].aggregated.success = false;
                results[runtime].aggregated.failures = 1;
              }
            }

            // Format results for comment display
            const denoResult = formatTestResult(results.deno.aggregated);
            const bunResult = formatTestResult(results.bun.aggregated);
            const nodeResult = formatTestResult(results.node.aggregated);
            const rustResult = formatTestResult(results.rust.aggregated);

            // Debug: List available coverage files
            console.log('Coverage files:');
            let coverageFiles = [];
            try {
              coverageFiles = fs.readdirSync('coverage-reports').filter(f => f.includes('lcov') || f.includes('info'));
              console.log('Coverage files found:', coverageFiles);
            } catch (error) {
              console.log('No coverage-reports directory:', error.message);
            }

            // Parse coverage files for actual percentages
            const denoCoverage = parseLCOVCoverage('coverage-reports/deno-lcov.info');
            const rustCoverage = parseLCOVCoverage('coverage-reports/rust.lcov');
            const bunCoverage = parseLCOVCoverage('coverage-reports/bun-lcov.info');
            const nodeCoverage = parseLCOVCoverage('coverage-reports/node-lcov.info');

            summary += `| 🦕 **Deno** | ${denoResult.status} | ${denoResult.text} | ${denoCoverage} |\n`;
            summary += `| 🥟 **Bun** | ${bunResult.status} | ${bunResult.text} | ${bunCoverage} |\n`;
            summary += `| 🟢 **Node.js** | ${nodeResult.status} | ${nodeResult.text} | ${nodeCoverage} |\n\n`;

            summary += `### Rust Library Tests\n`;
            summary += `Core library functionality tested in **Rust**:\n\n`;
            summary += `| Component | Status | Tests | Coverage |\n`;
            summary += `|-----------|--------|-------|----------|\n`;
            summary += `| 🦀 **Core Library** | ${rustResult.status} | ${rustResult.text} | ${rustCoverage} |\n\n`;

            // Add platform breakdown if we have platform-specific results
            let hasplatformResults = false;
            for (const runtime of runtimes) {
              if (Object.keys(results[runtime].platforms).length > 0) {
                hasplatformResults = true;
                break;
              }
            }

            if (hasplatformResults) {
              summary += `### 🖥️ Platform Test Results\n`;
              summary += `Cross-platform compatibility verified on **3 operating systems**:\n\n`;
              summary += `| Platform | Status | Deno | Bun | Node.js | Rust |\n`;
              summary += `|----------|--------|------|-----|---------|------|\n`;

              for (const platform of platforms) {
                const platformStatus = matrixResults === 'success' ? '✅' : '❌';

                // Get platform-specific results for each runtime
                const platformDeno = results.deno.platforms[platform];
                const platformBun = results.bun.platforms[platform];
                const platformNode = results.node.platforms[platform];
                const platformRust = results.rust.platforms[platform];

                const denoStatus = platformDeno ? (platformDeno.success ? '✅' : '❌') : '➖';
                const bunStatus = platformBun ? (platformBun.success ? '✅' : '❌') : '➖';
                const nodeStatus = platformNode ? (platformNode.success ? '✅' : '❌') : '➖';
                const rustStatus = platformRust ? (platformRust.success ? '✅' : '❌') : '➖';

                summary += `| ${platform === 'macOS' ? '🍎' : platform === 'Windows' ? '🪟' : '🐧'} **${platform}** | ${platformStatus} | ${denoStatus} | ${bunStatus} | ${nodeStatus} | ${rustStatus} |\n`;
              }
              summary += `\n`;
            }

            // List available artifacts
            summary += `### 📊 Test Artifacts Generated\n\n`;
            const testFiles = fs.existsSync('test-reports') ? fs.readdirSync('test-reports') : [];
            const allCoverageFiles = fs.existsSync('coverage-reports') ? fs.readdirSync('coverage-reports') : [];

            if (testFiles.length > 0) {
              summary += `**JUnit Reports**: ${testFiles.filter(f => f.endsWith('.xml')).join(', ')}\n\n`;
            }

            if (allCoverageFiles.length > 0) {
              summary += `**Coverage Reports**: ${allCoverageFiles.join(', ')}\n\n`;
            }

            // Overall status based on matrix results and test parsing
            const matrixSuccess = matrixResults === 'success';
            const testResultsSuccess = results.deno.aggregated.success &&
                                    results.bun.aggregated.success &&
                                    results.node.aggregated.success &&
                                    results.rust.aggregated.success;
            const overallSuccess = matrixSuccess && testResultsSuccess;
            const overallStatus = overallSuccess ? '✅ All tests passing' : '❌ Some tests failed';

            // Add platform information if matrix failed
            let platformInfo = '';
            if (!matrixSuccess) {
              platformInfo = ' (Matrix job failed on one or more platforms: Linux, macOS, Windows)';
            }

            summary += `**Overall Status**: ${overallStatus}${platformInfo}\n\n`;

            // Add timestamp and run info
            const timestamp = new Date().toISOString().replace('T', ' ').substring(0, 19);
            const runUrl = `https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`;
            summary += `*Last updated: ${timestamp} UTC | [View CI Run](${runUrl})* 🤖`;

            // Look for existing comment from this bot
            const COMMENT_IDENTIFIER = '## 🧪 Test Results';

            try {
              const comments = await github.rest.issues.listComments({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
              });

              // Find existing test results comment
              const existingComment = comments.data.find(comment =>
                comment.user.type === 'Bot' &&
                comment.body.includes(COMMENT_IDENTIFIER)
              );

              if (existingComment) {
                // Update existing comment
                await github.rest.issues.updateComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  comment_id: existingComment.id,
                  body: summary
                });
                console.log('Updated existing test results comment');
              } else {
                // Create new comment
                await github.rest.issues.createComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  issue_number: context.issue.number,
                  body: summary
                });
                console.log('Created new test results comment');
              }
            } catch (error) {
              console.error('Error managing comment:', error);
              // Fallback: try to create a new comment
              try {
                await github.rest.issues.createComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  issue_number: context.issue.number,
                  body: summary
                });
                console.log('Created fallback comment');
              } catch (fallbackError) {
                console.error('Fallback comment creation failed:', fallbackError);
              }
            }

      # Create status check based on test results
      - name: Create status check
        uses: actions/github-script@v8
        with:
          script: |
            const fs = require('fs');

            // Function to parse JUnit XML and check if tests passed
            function parseJUnit(xmlContent) {
              // Try multiple patterns for different JUnit formats
              let testMatch = xmlContent.match(/<testsuite[^>]*tests="(\d+)"[^>]*errors="(\d+)"[^>]*failures="(\d+)"/);

              if (!testMatch) {
                const testsMatch = xmlContent.match(/tests="(\d+)"/);
                const errorsMatch = xmlContent.match(/errors="(\d+)"/);
                const failuresMatch = xmlContent.match(/failures="(\d+)"/);

                if (testsMatch) {
                  const tests = parseInt(testsMatch[1]);
                  const errors = errorsMatch ? parseInt(errorsMatch[1]) : 0;
                  const failures = failuresMatch ? parseInt(failuresMatch[1]) : 0;
                  return { total: tests, errors, failures, passed: tests - errors - failures };
                }
              }

              if (!testMatch) {
                const testcases = (xmlContent.match(/<testcase/g) || []).length;
                const failures = (xmlContent.match(/<failure/g) || []).length;
                const errors = (xmlContent.match(/<error/g) || []).length;

                if (testcases > 0) {
                  return { total: testcases, errors, failures, passed: testcases - errors - failures };
                }
              }

              if (testMatch) {
                return {
                  total: parseInt(testMatch[1]),
                  errors: parseInt(testMatch[2]),
                  failures: parseInt(testMatch[3]),
                  passed: parseInt(testMatch[1]) - parseInt(testMatch[2]) - parseInt(testMatch[3])
                };
              }

              return { total: 0, errors: 0, failures: 0, passed: 0 };
            }

            // Aggregate test results from matrix job platforms
            const platformResults = {};
            const results = {};

            // Use matrix job results for status check (test files already parsed in comment generation)
            console.log('Using matrix job results for status check');

            // Get outcomes from build-and-test matrix jobs
            const matrixResults = ${{ toJSON(needs.build-and-test.result) }};
            const matrixOutputs = ${{ toJSON(needs.build-and-test.outputs) }};
            console.log('Matrix job result:', matrixResults);
            console.log('Matrix job outputs:', matrixOutputs);

            // Initialize platform tracking
            const platforms = ['Linux', 'macOS', 'Windows'];
            for (const platform of platforms) {
              platformResults[platform] = {
                overall: 'unknown',
                runtimes: {}
              };
            }

            // Process artifacts from each platform
            // The matrix job uploads artifacts with names like "test-reports-Linux", "test-reports-macOS", "test-reports-Windows"
            // After download-artifact with merge-multiple: true, all files are flattened to current directory

            // Initialize runtime results aggregation
            const runtimes = ['deno', 'bun', 'node', 'rust'];
            for (const runtime of runtimes) {
              results[runtime] = {
                platforms: {},
                aggregated: { success: false, total: 0, errors: 0, failures: 0, passed: 0 }
              };
            }

            // Look for test result files from all platforms
            // Collect all XML files from platform-specific directories
            const statusCheckTestFiles = [];
            for (const platform of platforms) {
              const platformPath = `reports/${platform}`;
              try {
                const platformFiles = fs.readdirSync(platformPath).filter(f => f.endsWith('.xml'));
                platformFiles.forEach(f => statusCheckTestFiles.push(`${platformPath}/${f}`));
              } catch (error) {
                console.log(`No test files found for ${platform}:`, error.message);
              }
            }
            console.log('Processing test files:', statusCheckTestFiles);

            // Process each test file and collect platform-specific results
            for (const fileName of statusCheckTestFiles) {
              try {
                const xml = fs.readFileSync(fileName, 'utf8');
                const parsed = parseJUnit(xml);

                // Determine which runtime and platform this file belongs to
                let runtime = null;
                let platform = null;

                if (fileName.includes('deno')) runtime = 'deno';
                else if (fileName.includes('bun')) runtime = 'bun';
                else if (fileName.includes('node')) runtime = 'node';
                else if (fileName.includes('cargo')) runtime = 'rust';

                // Extract platform from file path
                if (fileName.includes('Linux/')) platform = 'Linux';
                else if (fileName.includes('macOS/')) platform = 'macOS';
                else if (fileName.includes('Windows/')) platform = 'Windows';

                if (runtime && platform) {
                  // Store platform-specific results
                  if (!results[runtime].platforms[platform]) {
                    results[runtime].platforms[platform] = { success: false, total: 0, errors: 0, failures: 0, passed: 0 };
                  }

                  const platformResults = results[runtime].platforms[platform];
                  platformResults.total += parsed.total;
                  platformResults.errors += parsed.errors;
                  platformResults.failures += parsed.failures;
                  platformResults.passed += parsed.passed;
                  platformResults.success = platformResults.failures === 0 && platformResults.errors === 0;

                  console.log(`Processed ${fileName} for ${runtime} on ${platform}: ${parsed.passed}/${parsed.total} passed`);
                }
              } catch (error) {
                console.log(`Error processing ${fileName}:`, error.message);
              }
            }

            // Calculate aggregated results properly - don't sum across platforms,
            // instead determine if all platforms passed their tests
            for (const runtime of runtimes) {
              const platformKeys = Object.keys(results[runtime].platforms);
              if (platformKeys.length > 0) {
                // Use the test count from the first platform (all should be the same)
                const firstPlatform = platformKeys[0];
                const referenceResults = results[runtime].platforms[firstPlatform];

                // Check if ALL platforms passed their tests
                const allPlatformsPassed = platformKeys.every(platform =>
                  results[runtime].platforms[platform].success
                );

                results[runtime].aggregated = {
                  total: referenceResults.total,
                  errors: allPlatformsPassed ? 0 : 1,
                  failures: allPlatformsPassed ? 0 : 0,
                  passed: allPlatformsPassed ? referenceResults.total : referenceResults.total - 1,
                  success: allPlatformsPassed
                };
              }
            }

            // Fallback: if no results found, mark as failed
            for (const runtime of runtimes) {
              if (results[runtime].aggregated.total === 0) {
                results[runtime].aggregated.success = false;
                results[runtime].aggregated.failures = 1;
              }
            }

            // Status check uses matrix job results (detailed test parsing is done in comment generation)

            // Status check is based on matrix job results only
            const overallSuccess = matrixResults === 'success';

            console.log('Status check based on matrix job result:', matrixResults);

            const state = overallSuccess ? 'success' : 'failure';
            const description = overallSuccess ?
              'All matrix jobs passed' :
              'Matrix jobs failed';

            try {
              await github.rest.repos.createCommitStatus({
                owner: context.repo.owner,
                repo: context.repo.repo,
                sha: context.payload.pull_request?.head?.sha || context.sha,
                state: state,
                target_url: `https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`,
                description: description,
                context: 'Cross-Runtime Tests'
              });
              console.log(`Status check created: ${state} - ${description}`);
            } catch (error) {
              console.error('Failed to create status check:', error);
              // If status check creation fails, fail the job so PR gates work
              if (!overallSuccess) {
                core.setFailed(`Tests failed: ${description}`);
              }
            }

            // Also fail the job if tests failed so the workflow shows failure
            if (!overallSuccess) {
              core.setFailed(`Cross-runtime tests failed: ${description}`);
            }

      # Create code quality status check
      - name: Create code quality status check
        if: ${{ always() }}
        uses: actions/github-script@v8
        with:
          script: |
            // Get individual step outcomes from build job outputs
            const rustFmtResult = '${{ needs.code-quality.outputs.rust-fmt-result }}';
            const rustLintResult = '${{ needs.code-quality.outputs.rust-lint-result }}';
            const prettierFmtResult = '${{ needs.code-quality.outputs.prettier-fmt-result }}';
            const eslintResult = '${{ needs.code-quality.outputs.eslint-result }}';

            console.log('Code quality step results:');
            console.log(`- Rust fmt: ${rustFmtResult}`);
            console.log(`- Rust lint: ${rustLintResult}`);
            console.log(`- Prettier fmt: ${prettierFmtResult}`);
            console.log(`- ESLint: ${eslintResult}`);

            // Check if any code quality steps failed
            const codeQualityResults = {
              'Rust format': rustFmtResult,
              'Rust lint': rustLintResult,
              'Prettier format': prettierFmtResult,
              'ESLint': eslintResult
            };

            const failedChecks = Object.entries(codeQualityResults)
              .filter(([name, result]) => result === 'failure')
              .map(([name]) => name);

            const allPassed = failedChecks.length === 0;
            const state = allPassed ? 'success' : 'failure';

            let description;
            if (allPassed) {
              description = `All code quality checks passed (${Object.keys(codeQualityResults).length} checks)`;
            } else {
              description = `Code quality issues: ${failedChecks.join(', ')} failed`;
            }

            try {
              await github.rest.repos.createCommitStatus({
                owner: context.repo.owner,
                repo: context.repo.repo,
                sha: context.payload.pull_request?.head?.sha || context.sha,
                state: state,
                target_url: `https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`,
                description: description,
                context: 'Code Quality'
              });
              console.log(`Code quality status check created: ${state} - ${description}`);
            } catch (error) {
              console.error('Failed to create code quality status check:', error);
            }

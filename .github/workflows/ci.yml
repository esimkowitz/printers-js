name: Build

on:
  push:
    branches:
      - main
  pull_request_target:
    branches:
      - main

jobs:
  build:
    runs-on: ubuntu-latest
    env:
      PRINTERS_JS_SIMULATE: true # Force simulation mode for all tests
      CARGO_TERM_COLOR: always
    permissions: {}
    outputs:
      rust-fmt-result: ${{ steps.rust-fmt.outcome }}
      rust-lint-result: ${{ steps.rust-lint.outcome }}
      deno-fmt-result: ${{ steps.deno-fmt.outcome }}
      deno-lint-result: ${{ steps.deno-lint.outcome }}
      eslint-result: ${{ steps.eslint.outcome }}
      cargo-tests-result: ${{ steps.cargo-tests.outcome }}
      cargo-junit-result: ${{ steps.cargo-junit.outcome }}
      cross-runtime-tests-result: ${{ steps.cross-runtime-tests.outcome }}
    steps:
      - name: Checkout base branch
        if: ${{ github.event_name == 'push' }}
        uses: actions/checkout@v5

      - name: Checkout merge commit
        if: ${{ github.event_name == 'pull_request_target' }}
        uses: actions/checkout@v5
        with:
          ref: ${{ github.event.pull_request.head.ref }}

      # Speed up apt-get operations by disabling unnecessary man-db auto-update
      - name: Disable man-db auto-update
        run: |
          echo "man-db man-db/auto-update boolean false" | sudo debconf-set-selections
          sudo rm -f /var/lib/man-db/auto-update

      # Install system dependencies for Linux printing
      - name: Install CUPS development libraries
        run: |
          sudo apt-get update
          sudo apt-get install -y libcups2-dev pkg-config clang

      # Setup Rust toolchain for building native library
      - name: Setup Rust
        uses: actions-rust-lang/setup-rust-toolchain@v1

      # Cache Rust dependencies for faster builds
      - uses: Swatinem/rust-cache@v2
        with:
          prefix-key: ${{ runner.os }}-rust

      # Install cargo-llvm-cov for code coverage
      - name: Install cargo-llvm-cov
        uses: taiki-e/install-action@cargo-llvm-cov

      # Install cargo2junit for JUnit XML output from Cargo tests
      - name: Install cargo2junit
        run: cargo install cargo2junit

      # Setup Deno
      - uses: denoland/setup-deno@v2
        with:
          deno-version: v2.x # Run with latest stable Deno.
          cache: true

      # Setup Node.js for N-API module and testing
      - name: Setup Node.js
        uses: actions/setup-node@v5
        with:
          node-version: "20"
          cache: "npm"

      - name: Install NAPI-RS CLI
        run: npm install -g @napi-rs/cli

      # Setup Bun for cross-runtime testing
      - name: Setup Bun
        uses: oven-sh/setup-bun@v2
        with:
          bun-version: latest

      # Install Task runner
      - name: Setup Task
        uses: arduino/setup-task@v2
        with:
          version: 3.x
          repo-token: ${{ secrets.GITHUB_TOKEN }}

      # Code quality checks (continue on error to gather all results)
      - name: Check Rust formatting
        id: rust-fmt
        run: task fmt:rust:check
        continue-on-error: true

      - name: Run Rust linter (Clippy)
        id: rust-lint
        run: task lint:rust
        continue-on-error: true

      - name: Check Deno formatting
        id: deno-fmt
        run: task fmt:deno:check
        continue-on-error: true

      - name: Run Deno linter
        id: deno-lint
        run: task lint:deno
        continue-on-error: true

      # Install Node.js dependencies
      - name: Install Node.js dependencies
        run: npm install

      - name: Run ESLint for non-Deno files
        id: eslint
        run: task lint:node
        continue-on-error: true

      # Build all runtime libraries using our cross-platform TypeScript script
      - name: Build all runtime libraries
        run: task build

      # Run Cargo tests with coverage (Rust-level tests)
      - name: Run Cargo tests with coverage
        id: cargo-tests
        run: |
          mkdir -p test-results/coverage
          cargo llvm-cov --all-features --workspace --lcov --output-path test-results/coverage/rust.lcov test
        continue-on-error: true

      # Run Cargo tests and generate JUnit report
      - name: Generate Cargo test JUnit report
        id: cargo-junit
        run: |
          mkdir -p test-results
          # Run cargo tests and capture output
          cargo test --all-features --workspace > cargo_test_output.txt 2>&1 || true

          # Parse the test results from cargo output
          # Look for lines like "test result: ok. 16 passed; 0 failed; 0 ignored"
          RESULT_LINE=$(grep "test result:" cargo_test_output.txt | head -1)

          if [ -n "$RESULT_LINE" ]; then
            echo "Found test result line: $RESULT_LINE"

            # Extract numbers using more robust parsing
            PASSED=$(echo "$RESULT_LINE" | grep -oE "[0-9]+ passed" | grep -oE "[0-9]+" || echo "0")
            FAILED=$(echo "$RESULT_LINE" | grep -oE "[0-9]+ failed" | grep -oE "[0-9]+" || echo "0")
            IGNORED=$(echo "$RESULT_LINE" | grep -oE "[0-9]+ ignored" | grep -oE "[0-9]+" || echo "0")

            # Calculate total tests
            TOTAL=$((PASSED + FAILED + IGNORED))

            echo "Parsed: Total=$TOTAL, Passed=$PASSED, Failed=$FAILED, Ignored=$IGNORED"
          else
            echo "No test result line found, using defaults"
            TOTAL=0
            PASSED=0
            FAILED=0
            IGNORED=0
          fi

          # Generate JUnit XML
          echo '<?xml version="1.0" encoding="UTF-8"?>' > test-results/cargo.xml
          echo '<testsuites>' >> test-results/cargo.xml
          echo "  <testsuite name=\"Rust Cargo Tests\" tests=\"${TOTAL}\" failures=\"${FAILED}\" errors=\"0\" skipped=\"${IGNORED}\">" >> test-results/cargo.xml

          # Add individual test cases from the output
          grep "^test " cargo_test_output.txt | while IFS= read -r line; do
            # Extract test name and status
            TEST_NAME=$(echo "$line" | sed 's/test \(.*\) \.\.\. .*/\1/')
            TEST_STATUS=$(echo "$line" | sed 's/.*\.\.\. \(.*\)/\1/')

            if [ "$TEST_STATUS" = "ok" ]; then
              echo "    <testcase name=\"${TEST_NAME}\" classname=\"cargo\" />" >> test-results/cargo.xml
            elif [ "$TEST_STATUS" = "FAILED" ]; then
              echo "    <testcase name=\"${TEST_NAME}\" classname=\"cargo\">" >> test-results/cargo.xml
              echo "      <failure message=\"Test failed\" />" >> test-results/cargo.xml
              echo "    </testcase>" >> test-results/cargo.xml
            fi
          done

          echo '  </testsuite>' >> test-results/cargo.xml
          echo '</testsuites>' >> test-results/cargo.xml

          echo "Generated JUnit XML with $TOTAL tests"
          cat test-results/cargo.xml
        continue-on-error: true

      # Run comprehensive cross-runtime tests using our TypeScript script
      - name: Run cross-runtime tests
        id: cross-runtime-tests
        run: task test
        continue-on-error: true

      - name: Upload Test Reports
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: test-reports
          path: test-results/*.xml
          if-no-files-found: ignore

      - name: Upload Coverage Reports
        if: ${{ always() }}
        uses: actions/upload-artifact@v4
        with:
          name: coverage-reports
          path: test-results/coverage/*
          if-no-files-found: ignore

  report:
    if: ${{ always() && github.event_name == 'pull_request_target'}}
    runs-on: ubuntu-latest
    needs: build
    permissions:
      contents: read
      pull-requests: write
      actions: read
      checks: write
      statuses: write
    steps:
      # Speed up apt-get operations by disabling unnecessary man-db auto-update
      - name: Disable man-db auto-update
        run: |
          echo "man-db man-db/auto-update boolean false" | sudo debconf-set-selections
          sudo rm -f /var/lib/man-db/auto-update

      - uses: actions/download-artifact@v5
        if: ${{ always() }}
        continue-on-error: true
        with:
          pattern: "*-reports"
          merge-multiple: true

      # Create combined coverage report from all runtimes
      - name: Combine coverage reports
        run: |
          # Coverage files are flattened in current directory after artifact download

          # Create coverage-reports directory for organized output
          mkdir -p coverage-reports

          # Create combined report from files in current directory
          if [ -f rust.lcov ]; then
            cp rust.lcov coverage-reports/rust.lcov
            cp rust.lcov coverage-reports/combined.lcov
          else
            touch coverage-reports/combined.lcov
          fi

          if [ -f deno-lcov.info ]; then
            cp deno-lcov.info coverage-reports/deno-lcov.info
            cat deno-lcov.info >> coverage-reports/combined.lcov
          fi

          # Copy other coverage files to organized directory
          if [ -f bun-lcov.info ]; then
            cp bun-lcov.info coverage-reports/bun-lcov.info
          fi

          if [ -f node-lcov.info ]; then
            cp node-lcov.info coverage-reports/node-lcov.info
          fi

          # Clean up the combined coverage file to remove problematic entries
          # This filters out TLA:GNC lines which can cause genhtml errors
          if [ -f coverage-reports/combined.lcov ] && [ -s coverage-reports/combined.lcov ]; then
            cd coverage-reports
            grep -v "TLA:GNC" combined.lcov > combined_clean.lcov || cp combined.lcov combined_clean.lcov
            mv combined_clean.lcov combined.lcov
            cd ..
          fi

      # Add/update dynamic test summary comment based on actual results
      - name: Update Test Summary Comment
        uses: actions/github-script@v8
        with:
          script: |
            const fs = require('fs');
            const path = require('path');

            // Function to parse JUnit XML (handles different formats)
            function parseJUnit(xmlContent) {
              // Try multiple patterns for different JUnit formats
              // Pattern 1: Standard JUnit with tests, errors, failures attributes
              let testMatch = xmlContent.match(/<testsuite[^>]*tests="(\d+)"[^>]*errors="(\d+)"[^>]*failures="(\d+)"/);

              // Pattern 2: Alternative format with different attribute order
              if (!testMatch) {
                const testsMatch = xmlContent.match(/tests="(\d+)"/);
                const errorsMatch = xmlContent.match(/errors="(\d+)"/);
                const failuresMatch = xmlContent.match(/failures="(\d+)"/);

                if (testsMatch) {
                  const tests = parseInt(testsMatch[1]);
                  const errors = errorsMatch ? parseInt(errorsMatch[1]) : 0;
                  const failures = failuresMatch ? parseInt(failuresMatch[1]) : 0;
                  return {
                    total: tests,
                    errors: errors,
                    failures: failures,
                    passed: tests - errors - failures
                  };
                }
              }

              // Pattern 3: Count individual testcase elements
              if (!testMatch) {
                const testcases = (xmlContent.match(/<testcase/g) || []).length;
                const failures = (xmlContent.match(/<failure/g) || []).length;
                const errors = (xmlContent.match(/<error/g) || []).length;

                if (testcases > 0) {
                  return {
                    total: testcases,
                    errors: errors,
                    failures: failures,
                    passed: testcases - errors - failures
                  };
                }
              }

              if (testMatch) {
                return {
                  total: parseInt(testMatch[1]),
                  errors: parseInt(testMatch[2]),
                  failures: parseInt(testMatch[3]),
                  passed: parseInt(testMatch[1]) - parseInt(testMatch[2]) - parseInt(testMatch[3])
                };
              }

              return { total: 0, errors: 0, failures: 0, passed: 0 };
            }

            // Function to format test result
            function formatTestResult(result) {
              const total = result.passed + result.failures + result.errors;
              const status = result.failures === 0 && result.errors === 0 ? 'âœ…' : 'âŒ';
              return { status, text: `${result.passed}/${total} passed` };
            }

            // Parse LCOV coverage file to get coverage percentage
            function parseLCOVCoverage(filePath) {
              try {
                if (!fs.existsSync(filePath)) {
                  return 'No coverage';
                }

                const lcovContent = fs.readFileSync(filePath, 'utf8');

                // Count total and hit lines
                let totalLines = 0;
                let hitLines = 0;

                const lines = lcovContent.split('\n');
                for (const line of lines) {
                  if (line.startsWith('DA:')) {
                    // DA:line_number,hit_count
                    totalLines++;
                    const hitCount = parseInt(line.split(',')[1] || '0');
                    if (hitCount > 0) {
                      hitLines++;
                    }
                  }
                }

                if (totalLines === 0) {
                  return 'No data';
                }

                const percentage = Math.round((hitLines / totalLines) * 100);
                return `${percentage}% (${hitLines}/${totalLines})`;
              } catch (error) {
                console.log('Error parsing LCOV:', error.message);
                return 'Parse error';
              }
            }

            // Generate the summary content
            let summary = `## ðŸ§ª Test Results\n\n`;

            summary += `### JavaScript Runtime Tests\n`;
            summary += `Cross-runtime compatibility verified across **3 JavaScript runtimes**:\n\n`;
            summary += `| Runtime | Status | Tests | Coverage |\n`;
            summary += `|---------|--------|-------|----------|\n`;

            // Read Deno test results
            let denoResult = { status: 'â“', text: 'Not found' };
            try {
              if (fs.existsSync('deno-test-results.xml')) {
                const denoXml = fs.readFileSync('deno-test-results.xml', 'utf8');
                console.log('Deno XML preview:', denoXml.substring(0, 500));
                const parsed = parseJUnit(denoXml);
                console.log('Deno parsed result:', parsed);
                denoResult = formatTestResult(parsed);
              } else {
                console.log('Deno test results file not found');
              }
            } catch (e) {
              console.log('Error reading Deno results:', e.message);
            }

            // Read Rust/Cargo test results
            let rustResult = { status: 'â“', text: 'Not found' };
            try {
              if (fs.existsSync('cargo.xml')) {
                const cargoXml = fs.readFileSync('cargo.xml', 'utf8');
                const parsed = parseJUnit(cargoXml);
                rustResult = formatTestResult(parsed);
              }
            } catch (e) {
              console.log('Error reading Rust results:', e.message);
            }

            // Read Bun test results
            let bunResult = { status: 'â“', text: 'Not found' };
            try {
              if (fs.existsSync('bun-test-results.xml')) {
                const bunXml = fs.readFileSync('bun-test-results.xml', 'utf8');
                const parsed = parseJUnit(bunXml);
                bunResult = formatTestResult(parsed);
              }
            } catch (e) {
              console.log('Error reading Bun results:', e.message);
            }

            // Read Node.js test results
            let nodeResult = { status: 'â“', text: 'Not found' };
            try {
              if (fs.existsSync('node-test-results.xml')) {
                const nodeXml = fs.readFileSync('node-test-results.xml', 'utf8');
                const parsed = parseJUnit(nodeXml);
                nodeResult = formatTestResult(parsed);
              }
            } catch (e) {
              console.log('Error reading Node.js results:', e.message);
            }

            // Debug: List available files
            console.log('Available files in current directory:');
            const files = fs.readdirSync('.').filter(f => f.includes('lcov') || f.includes('info'));
            console.log('Coverage-related files:', files);

            // Parse coverage files for actual percentages (flattened after artifact download)
            const denoCoverage = parseLCOVCoverage('deno-lcov.info');
            const rustCoverage = parseLCOVCoverage('rust.lcov');
            const bunCoverage = parseLCOVCoverage('bun-lcov.info');
            const nodeCoverage = parseLCOVCoverage('node-lcov.info');

            summary += `| ðŸ¦• **Deno** | ${denoResult.status} | ${denoResult.text} | ${denoCoverage} |\n`;
            summary += `| ðŸ¥Ÿ **Bun** | ${bunResult.status} | ${bunResult.text} | ${bunCoverage} |\n`;
            summary += `| ðŸŸ¢ **Node.js** | ${nodeResult.status} | ${nodeResult.text} | ${nodeCoverage} |\n\n`;

            summary += `### Rust Library Tests\n`;
            summary += `Core library functionality tested in **Rust**:\n\n`;
            summary += `| Component | Status | Tests | Coverage |\n`;
            summary += `|-----------|--------|-------|----------|\n`;
            summary += `| ðŸ¦€ **Core Library** | ${rustResult.status} | ${rustResult.text} | ${rustCoverage} |\n\n`;

            // List available artifacts
            summary += `### ðŸ“Š Test Artifacts Generated\n\n`;
            const testFiles = fs.existsSync('test-reports') ? fs.readdirSync('test-reports') : [];
            const coverageFiles = fs.existsSync('coverage-reports') ? fs.readdirSync('coverage-reports') : [];

            if (testFiles.length > 0) {
              summary += `**JUnit Reports**: ${testFiles.filter(f => f.endsWith('.xml')).join(', ')}\n\n`;
            }

            if (coverageFiles.length > 0) {
              summary += `**Coverage Reports**: ${coverageFiles.join(', ')}\n\n`;
            }

            // Overall status
            const overallSuccess = denoResult.status === 'âœ…' && rustResult.status === 'âœ…' && bunResult.status === 'âœ…' && nodeResult.status === 'âœ…';
            const overallStatus = overallSuccess ? 'âœ… All tests passing' : 'âŒ Some tests failed';
            summary += `**Overall Status**: ${overallStatus}\n\n`;

            // Add timestamp and run info
            const timestamp = new Date().toISOString().replace('T', ' ').substring(0, 19);
            const runUrl = `https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`;
            summary += `*Last updated: ${timestamp} UTC | [View CI Run](${runUrl})* ðŸ¤–`;

            // Look for existing comment from this bot
            const COMMENT_IDENTIFIER = '## ðŸ§ª Test Results';

            try {
              const comments = await github.rest.issues.listComments({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
              });

              // Find existing test results comment
              const existingComment = comments.data.find(comment =>
                comment.user.type === 'Bot' &&
                comment.body.includes(COMMENT_IDENTIFIER)
              );

              if (existingComment) {
                // Update existing comment
                await github.rest.issues.updateComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  comment_id: existingComment.id,
                  body: summary
                });
                console.log('Updated existing test results comment');
              } else {
                // Create new comment
                await github.rest.issues.createComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  issue_number: context.issue.number,
                  body: summary
                });
                console.log('Created new test results comment');
              }
            } catch (error) {
              console.error('Error managing comment:', error);
              // Fallback: try to create a new comment
              try {
                await github.rest.issues.createComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  issue_number: context.issue.number,
                  body: summary
                });
                console.log('Created fallback comment');
              } catch (fallbackError) {
                console.error('Fallback comment creation failed:', fallbackError);
              }
            }

      # Create status check based on test results
      - name: Create status check
        uses: actions/github-script@v8
        with:
          script: |
            const fs = require('fs');

            // Function to parse JUnit XML and check if tests passed
            function parseJUnit(xmlContent) {
              // Try multiple patterns for different JUnit formats
              let testMatch = xmlContent.match(/<testsuite[^>]*tests="(\d+)"[^>]*errors="(\d+)"[^>]*failures="(\d+)"/);

              if (!testMatch) {
                const testsMatch = xmlContent.match(/tests="(\d+)"/);
                const errorsMatch = xmlContent.match(/errors="(\d+)"/);
                const failuresMatch = xmlContent.match(/failures="(\d+)"/);

                if (testsMatch) {
                  const tests = parseInt(testsMatch[1]);
                  const errors = errorsMatch ? parseInt(errorsMatch[1]) : 0;
                  const failures = failuresMatch ? parseInt(failuresMatch[1]) : 0;
                  return { total: tests, errors, failures, passed: tests - errors - failures };
                }
              }

              if (!testMatch) {
                const testcases = (xmlContent.match(/<testcase/g) || []).length;
                const failures = (xmlContent.match(/<failure/g) || []).length;
                const errors = (xmlContent.match(/<error/g) || []).length;

                if (testcases > 0) {
                  return { total: testcases, errors, failures, passed: testcases - errors - failures };
                }
              }

              if (testMatch) {
                return {
                  total: parseInt(testMatch[1]),
                  errors: parseInt(testMatch[2]),
                  failures: parseInt(testMatch[3]),
                  passed: parseInt(testMatch[1]) - parseInt(testMatch[2]) - parseInt(testMatch[3])
                };
              }

              return { total: 0, errors: 0, failures: 0, passed: 0 };
            }

            // Check test results for each runtime
            const results = {};

            // Debug: List all available files
            console.log('Available files in current directory:');
            console.log(fs.readdirSync('.').filter(f => f.endsWith('.xml')));

            // Check Deno results (files are flattened after artifact download)
            if (fs.existsSync('deno-test-results.xml')) {
              const xml = fs.readFileSync('deno-test-results.xml', 'utf8');
              const parsed = parseJUnit(xml);
              results.deno = { success: parsed.failures === 0 && parsed.errors === 0, ...parsed };
            } else {
              results.deno = { success: false, total: 0, errors: 0, failures: 1, passed: 0 };
            }

            // Check Bun results
            if (fs.existsSync('bun-test-results.xml')) {
              const xml = fs.readFileSync('bun-test-results.xml', 'utf8');
              const parsed = parseJUnit(xml);
              results.bun = { success: parsed.failures === 0 && parsed.errors === 0, ...parsed };
            } else {
              results.bun = { success: false, total: 0, errors: 0, failures: 1, passed: 0 };
            }

            // Check Node.js results
            if (fs.existsSync('node-test-results.xml')) {
              const xml = fs.readFileSync('node-test-results.xml', 'utf8');
              const parsed = parseJUnit(xml);
              results.node = { success: parsed.failures === 0 && parsed.errors === 0, ...parsed };
            } else {
              results.node = { success: false, total: 0, errors: 0, failures: 1, passed: 0 };
            }

            // Rust tests are optional since there are no unit tests defined
            results.rust = { success: true, total: 0, errors: 0, failures: 0, passed: 0 };
            if (fs.existsSync('cargo.xml')) {
              const xml = fs.readFileSync('cargo.xml', 'utf8');
              const parsed = parseJUnit(xml);
              results.rust = { success: parsed.failures === 0 && parsed.errors === 0, ...parsed };
            }

            // Check if cross-runtime test step succeeded
            const crossRuntimeTestsResult = '${{ needs.build.outputs.cross-runtime-tests-result }}';
            const cargoTestsResult = '${{ needs.build.outputs.cargo-tests-result }}';

            console.log(`Cross-runtime tests step result: ${crossRuntimeTestsResult}`);
            console.log(`Cargo tests step result: ${cargoTestsResult}`);

            // Determine overall success (both step results and parsed test results)
            const stepSuccess = crossRuntimeTestsResult === 'success' && cargoTestsResult === 'success';
            const testResultsSuccess = results.deno.success && results.bun.success && results.node.success && results.rust.success;
            const overallSuccess = stepSuccess && testResultsSuccess;

            const totalTests = results.deno.total + results.bun.total + results.node.total + results.rust.total;
            const totalPassed = results.deno.passed + results.bun.passed + results.node.passed + results.rust.passed;
            const totalFailures = results.deno.failures + results.bun.failures + results.node.failures + results.rust.failures;
            const totalErrors = results.deno.errors + results.bun.errors + results.node.errors + results.rust.errors;

            console.log('Test Results Summary:');
            console.log(`- Deno: ${results.deno.passed}/${results.deno.total} passed (success: ${results.deno.success})`);
            console.log(`- Bun: ${results.bun.passed}/${results.bun.total} passed (success: ${results.bun.success})`);
            console.log(`- Node.js: ${results.node.passed}/${results.node.total} passed (success: ${results.node.success})`);
            console.log(`- Rust: ${results.rust.passed}/${results.rust.total} passed (success: ${results.rust.success})`);
            console.log(`- Overall: ${totalPassed}/${totalTests} passed (success: ${overallSuccess})`);

            const state = overallSuccess ? 'success' : 'failure';
            let description;
            if (overallSuccess) {
              description = `All tests passed (${totalPassed}/${totalTests})`;
            } else {
              const issues = [];
              if (!stepSuccess) {
                const failedSteps = [];
                if (crossRuntimeTestsResult !== 'success') failedSteps.push('cross-runtime');
                if (cargoTestsResult !== 'success') failedSteps.push('cargo');
                issues.push(`${failedSteps.join(', ')} test steps failed`);
              }
              if (!testResultsSuccess) {
                issues.push(`${totalFailures} failures, ${totalErrors} errors`);
              }
              description = `Tests failed: ${issues.join('; ')} (${totalPassed}/${totalTests} passed)`;
            }

            try {
              await github.rest.repos.createCommitStatus({
                owner: context.repo.owner,
                repo: context.repo.repo,
                sha: context.payload.pull_request?.head?.sha || context.sha,
                state: state,
                target_url: `https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`,
                description: description,
                context: 'Cross-Runtime Tests'
              });
              console.log(`Status check created: ${state} - ${description}`);
            } catch (error) {
              console.error('Failed to create status check:', error);
              // If status check creation fails, fail the job so PR gates work
              if (!overallSuccess) {
                core.setFailed(`Tests failed: ${description}`);
              }
            }

            // Also fail the job if tests failed so the workflow shows failure
            if (!overallSuccess) {
              core.setFailed(`Cross-runtime tests failed: ${description}`);
            }

      # Create code quality status check
      - name: Create code quality status check
        if: ${{ always() }}
        uses: actions/github-script@v8
        with:
          script: |
            // Get individual step outcomes from build job outputs
            const rustFmtResult = '${{ needs.build.outputs.rust-fmt-result }}';
            const rustLintResult = '${{ needs.build.outputs.rust-lint-result }}';
            const denoFmtResult = '${{ needs.build.outputs.deno-fmt-result }}';
            const denoLintResult = '${{ needs.build.outputs.deno-lint-result }}';
            const eslintResult = '${{ needs.build.outputs.eslint-result }}';

            console.log('Code quality step results:');
            console.log(`- Rust fmt: ${rustFmtResult}`);
            console.log(`- Rust lint: ${rustLintResult}`);
            console.log(`- Deno fmt: ${denoFmtResult}`);
            console.log(`- Deno lint: ${denoLintResult}`);
            console.log(`- ESLint: ${eslintResult}`);

            // Check if any code quality steps failed
            const codeQualityResults = {
              'Rust format': rustFmtResult,
              'Rust lint': rustLintResult,
              'Deno format': denoFmtResult,
              'Deno lint': denoLintResult,
              'ESLint': eslintResult
            };

            const failedChecks = Object.entries(codeQualityResults)
              .filter(([name, result]) => result === 'failure')
              .map(([name]) => name);

            const allPassed = failedChecks.length === 0;
            const state = allPassed ? 'success' : 'failure';

            let description;
            if (allPassed) {
              description = `All code quality checks passed (${Object.keys(codeQualityResults).length} checks)`;
            } else {
              description = `Code quality issues: ${failedChecks.join(', ')} failed`;
            }

            try {
              await github.rest.repos.createCommitStatus({
                owner: context.repo.owner,
                repo: context.repo.repo,
                sha: context.payload.pull_request?.head?.sha || context.sha,
                state: state,
                target_url: `https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`,
                description: description,
                context: 'Code Quality'
              });
              console.log(`Code quality status check created: ${state} - ${description}`);
            } catch (error) {
              console.error('Failed to create code quality status check:', error);
            }
